{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Testing the Deep Reinforcement Implementation\n\nPlease take a look at the game played below. I tried to get a game as descriptive of the models capabilities as possible. While it is not very good (not even close to the skill of the adversarial search), it did develop some game sense in its ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"Like the \"trying deep reinforcement\" notebook, skip this block, it just has all the implementation needed to run the game. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Board is 8x8 numpy array\n# 0 = no piece\n# 1 = X piece\n# 2 = O piece\n\nblank = 0\nX = 1\nO = 2\n\nsymbol = [' ','X','O']\n\nN = 8      \n\ndef getEmptyBoard():                              # use this function to create a fresh empty board\n    return np.zeros((N,N)).astype(int)\n\n# This will be used to indicate an error when you try to make a move in a column that is already full\n\nERROR = -1\n\n# Check for error: use this function ONLY, since numpy arrays work strangely with comparisons\n\ndef isError(B):\n    if type(B) == int:\n        return B == ERROR\n    else:\n        return False\n\n# Print out a human-readable version of the board, can indent if want to trace through the recursion\n\ndef printBoard(B,ind=0):\n    indent = '\\t'*ind\n    if isError(B):\n        print(indent,\"ERROR: Overflow in column.\")\n        return\n    print(indent,'  0 1 2 3 4 5 6 7')\n    print(indent,'-------------------')\n    for row in range(N):\n        print(indent,'|',end='')\n        for col in range(N):\n            print(' '+ symbol[B[row][col]],end='')\n        print(' |')\n    print(indent,'-------------------')\n    \n\n\n# This function should make the indicated move on the input board, and return that board, or ERROR (-1)\n# if there is no room in the column of the move.  Note that you are changing the original board\n# IN PLACE, but also returning it, so you can indicate the error by returning ERROR (-1).\n# Do NOT make a copy, as that is very inefficient!\n\n# player is 1 (X) or 2 (O); 0 <= move <= 7; board is 8x8 numpy array as shown in first code cell.\n# If move is illegal (either outside range 0..7) or there is no room in that column, return ERROR\n\ndef illegalMove(m):\n    return not(0 <= m <= 7)\n\ndef noRoomInColumn(move,board):\n    return board[0][move] != blank\n\ndef dropPiece(player, move, board):\n    if illegalMove(move) or noRoomInColumn(move, board):\n        return board\n    row = 7\n    while board[row][move] != blank:\n        row -= 1\n    board[row][move] = player\n    return board\n\n\n\nimport numpy as np\n\ndef checkWin(player, board):\n    # Check horizontal, vertical, and diagonal lines for a win\n    pattern = np.array([player] * 4)\n    \n    # Horizontal\n    for row in range(8):\n        for col in range(5):\n            if np.array_equal(board[row, col:col + 4], pattern):\n                return player\n\n    # Vertical\n    for col in range(8):\n        for row in range(5):\n            if np.array_equal(board[row:row + 4, col], pattern):\n                return player\n\n    # Diagonal (down-right)\n    for row in range(5):\n        for col in range(5):\n            if np.array_equal(board[range(row, row + 4), range(col, col + 4)], pattern):\n                return player\n\n    # Diagonal (up-right)\n    for row in range(3, 8):\n        for col in range(5):\n            if np.array_equal(board[range(row, row - 4, -1), range(col, col + 4)], pattern):\n                return player\n\n    return 0\n\nimport sys\n\n\n# Return evaluation of the board from O's point of view\n\ndef evaluate(board, player):\n    opponent = 3 - player\n    my_score = score(board, player)\n    opponent_score = score(board, opponent)\n    return my_score - 0.5 * opponent_score\n\ndef score(board, player):\n    width=8\n    height=8\n    total = 0\n    # Horizontal\n    for y in range(height):\n        total += eval_dir(board, 0, y, 1, 0, player)\n    # Vertical\n    for x in range(width):\n        total += eval_dir(board, x, 0, 0, 1, player)\n    # Diagonal from bottom-left to top-right\n    for x in range(width):\n        total += eval_dir(board, x, 0, 1, 1, player)\n    for y in range(1, height):\n        total += eval_dir(board, 0, y, 1, 1, player)\n    # Diagonal from top-left to bottom-right\n    for x in range(width):\n        total += eval_dir(board, x, height - 1, 1, -1, player)\n    for y in range(height - 2, -1, -1):\n        total += eval_dir(board, 0, y, 1, -1, player)\n    return total\n\ndef eval_dir(board, start_x, start_y, dx, dy, player):\n    consecutive = 0\n    blocks = 0\n    for i in range(8):\n        if start_x + i*dx < 0 or start_y + i*dy < 0 or start_x + i*dx >= len(board) or start_y + i*dy >= len(board[0]):\n            # Out of bounds\n            blocks += 1\n            continue\n        if board[start_y + i*dy][start_x + i*dx] == player:\n            # Consecutive player piece\n            consecutive += 1\n        elif board[start_y + i*dy][start_x + i*dx] != 0:\n            # Piece from the other player\n            blocks += 1\n            break\n        else:\n            # Empty space\n            break\n    if blocks == 2 or consecutive == 0:\n        return 0\n    switcher = {\n        1: 1,\n        2: 10,\n        3: 1000,\n        4: sys.maxsize, # Increase the score for a win\n    }\n    return switcher.get(consecutive, 0)\n\nmaxDepth = 3\n\ndef minMax(board, player, depth, alpha, beta):\n    if depth >= maxDepth or checkWin(player, board) or checkWin((player % 2) + 1, board): \n        return (evaluate(board, player), None)\n\n    moves = getAvailableMoves(board)\n    best_move = None\n    \n    if player == O:  # Maximizing player\n        maxEval = -np.inf\n        for move in moves:\n            row = get_row(board, move)\n            if row == -1: continue\n            board[row][move] = player\n            if checkWin(player, board):\n                board[row][move] = blank  # undo move\n                return (np.inf, move)  # Found a winning move, no need to consider other moves\n            val, _ = minMax(board, X, depth + 1, alpha, beta)\n            board[row][move] = blank  # undo move\n            if val > maxEval:\n                maxEval = val\n                best_move = move\n            alpha = max(alpha, val)\n            if beta <= alpha:\n                break\n        return maxEval, best_move\n\n    else:  # Minimizing player\n        minEval = np.inf\n        for move in moves:\n            row = get_row(board, move)\n            if row == -1: continue\n            board[row][move] = player\n            if checkWin(player, board):\n                board[row][move] = blank  # undo move\n                return (-np.inf, move)  # Found a winning move for the opponent, no need to consider other moves\n            val, _ = minMax(board, O, depth + 1, alpha, beta)\n            board[row][move] = blank  # undo move\n            if val < minEval:\n                minEval = val\n                best_move = move\n            beta = min(beta, val)\n            if beta <= alpha:\n                break\n        return minEval, best_move\n\n\n    \ndef get_row(board,col):\n    for i in range (0,8):\n        if board[i][col] != 0:\n            return i - 1\n    return 7\n\ndef getAvailableMoves(board):\n    return [i for i in range(8) if get_row(board, i) != -1]\n\ndef getRandomBoard():\n    board = getEmptyBoard()\n    moves_played = random.randint(0, 60) + 1\n    if moves_played % 2 == 1:\n        moves_played -= 1\n    player = 1\n    move_history = []\n\n    for _ in range(moves_played):\n        while True:\n            move = random.choice(getAvailableMoves(board))\n            board_temp = dropPiece(player, move, board.copy())  # use a temp board\n            if not checkWin(player, board_temp):\n                # If the move doesn't cause a win, make it on the real board\n                board = dropPiece(player, move, board)\n                move_history.append(move)\n                player = (player % 2) + 1\n                break\n            elif len(move_history) > 0 or abs(minMax(board,player,0,-sys.maxsize,sys.maxsize)[0]) == sys.maxsize: \n                # If the move causes a win and it's not the first move, undo the last move and try again\n                undoLastMove(board, move_history.pop())\n            else:\n                # If it's the first move and it causes a win, just try a different move\n                continue\n\n    return board\n\ndef isBoardFull(board):\n    return not np.any(board == blank)\n\ndef undoLastMove(board, move):\n    \"\"\"Remove the top-most piece from the given column.\"\"\"\n    for row in range(N):\n        if board[row][move] != blank:\n            board[row][move] = blank\n            break","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-28T20:58:23.700181Z","iopub.execute_input":"2023-04-28T20:58:23.700746Z","iopub.status.idle":"2023-04-28T20:58:23.748959Z","shell.execute_reply.started":"2023-04-28T20:58:23.700696Z","shell.execute_reply":"2023-04-28T20:58:23.747453Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nimport numpy as np\nimport random\n\n# Load the model\nmodel = tf.keras.models.load_model('/kaggle/input/reinforcement-nn/reinforcement_cnn.h5')","metadata":{"execution":{"iopub.status.busy":"2023-04-28T20:58:23.751341Z","iopub.execute_input":"2023-04-28T20:58:23.751803Z","iopub.status.idle":"2023-04-28T20:58:23.828910Z","shell.execute_reply.started":"2023-04-28T20:58:23.751765Z","shell.execute_reply":"2023-04-28T20:58:23.827768Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Load the model\nmodel = tf.keras.models.load_model('/kaggle/input/reinforcement-nn/reinforcement_cnn.h5')\n\n# Initialize the board\nboard = getRandomBoard()\n\n# Player 'X' (AI) goes first, player 'O' (human) goes second\nplayer = X\n\nwhile True:\n    # Print the current board\n    printBoard(board)\n\n    if player == 1:\n        # AI's turn\n        move_probabilities = model.predict(np.array(board).reshape(-1, 8, 8))\n        move = np.argmax(move_probabilities[0])\n        \n        # If the move is illegal or there's no room, choose another move\n        while illegalMove(move) or noRoomInColumn(move, board):\n            move_probabilities[0][move] = 0  # Set the probability of the illegal move to 0\n            move = np.argmax(move_probabilities[0])\n            \n        print(f\"AI's move: {move}\")\n\n        # Apply the move and get the new board\n        board = dropPiece(1, move, board)\n\n        # Check if the AI has won\n        if checkWin(1, board):\n            printBoard(board)\n            print(\"AI has won!\")\n            break\n\n    else:\n        # Human's turn\n        move = int(input(\"Enter your move (0-7): \"))\n\n        # If the move is illegal or there's no room, ask for another move\n        while illegalMove(move) or noRoomInColumn(move, board):\n            print(\"Invalid move.\")\n            move = int(input(\"Enter your move (0-7): \"))\n\n        # Apply the move and get the new board\n        board = dropPiece(2, move, board)\n\n        # Check if the human has won\n        if checkWin(2, board):\n            printBoard(board)\n            print(\"You have won!\")\n            break\n\n    # Switch players\n    player = 2 if player == 1 else 1","metadata":{"execution":{"iopub.status.busy":"2023-04-28T20:58:23.830227Z","iopub.execute_input":"2023-04-28T20:58:23.831256Z","iopub.status.idle":"2023-04-28T20:59:34.213319Z","shell.execute_reply.started":"2023-04-28T20:58:23.831211Z","shell.execute_reply":"2023-04-28T20:59:34.211897Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"   0 1 2 3 4 5 6 7\n -------------------\n |                 |\n |         O       |\n |   X     O       |\n |   X     X       |\n | X O     O       |\n | O O     O     O |\n | X X     X     O |\n | X O X X X O   O |\n -------------------\n1/1 [==============================] - 0s 58ms/step\nAI's move: 7\n   0 1 2 3 4 5 6 7\n -------------------\n |                 |\n |         O       |\n |   X     O       |\n |   X     X       |\n | X O     O     X |\n | O O     O     O |\n | X X     X     O |\n | X O X X X O   O |\n -------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your move (0-7):  6\n"},{"name":"stdout","text":"   0 1 2 3 4 5 6 7\n -------------------\n |                 |\n |         O       |\n |   X     O       |\n |   X     X       |\n | X O     O     X |\n | O O     O     O |\n | X X     X     O |\n | X O X X X O O O |\n -------------------\n1/1 [==============================] - 0s 24ms/step\nAI's move: 7\n   0 1 2 3 4 5 6 7\n -------------------\n |                 |\n |         O       |\n |   X     O       |\n |   X     X     X |\n | X O     O     X |\n | O O     O     O |\n | X X     X     O |\n | X O X X X O O O |\n -------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your move (0-7):  6\n"},{"name":"stdout","text":"   0 1 2 3 4 5 6 7\n -------------------\n |                 |\n |         O       |\n |   X     O       |\n |   X     X     X |\n | X O     O     X |\n | O O     O     O |\n | X X     X   O O |\n | X O X X X O O O |\n -------------------\n1/1 [==============================] - 0s 24ms/step\nAI's move: 7\n   0 1 2 3 4 5 6 7\n -------------------\n |                 |\n |         O       |\n |   X     O     X |\n |   X     X     X |\n | X O     O     X |\n | O O     O     O |\n | X X     X   O O |\n | X O X X X O O O |\n -------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your move (0-7):  7\n"},{"name":"stdout","text":"   0 1 2 3 4 5 6 7\n -------------------\n |                 |\n |         O     O |\n |   X     O     X |\n |   X     X     X |\n | X O     O     X |\n | O O     O     O |\n | X X     X   O O |\n | X O X X X O O O |\n -------------------\n1/1 [==============================] - 0s 23ms/step\nAI's move: 7\n   0 1 2 3 4 5 6 7\n -------------------\n |               X |\n |         O     O |\n |   X     O     X |\n |   X     X     X |\n | X O     O     X |\n | O O     O     O |\n | X X     X   O O |\n | X O X X X O O O |\n -------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your move (0-7):  5\n"},{"name":"stdout","text":"   0 1 2 3 4 5 6 7\n -------------------\n |               X |\n |         O     O |\n |   X     O     X |\n |   X     X     X |\n | X O     O     X |\n | O O     O     O |\n | X X     X O O O |\n | X O X X X O O O |\n -------------------\n1/1 [==============================] - 0s 23ms/step\nAI's move: 6\n   0 1 2 3 4 5 6 7\n -------------------\n |               X |\n |         O     O |\n |   X     O     X |\n |   X     X     X |\n | X O     O     X |\n | O O     O   X O |\n | X X     X O O O |\n | X O X X X O O O |\n -------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your move (0-7):  5\n"},{"name":"stdout","text":"   0 1 2 3 4 5 6 7\n -------------------\n |               X |\n |         O     O |\n |   X     O     X |\n |   X     X     X |\n | X O     O     X |\n | O O     O O X O |\n | X X     X O O O |\n | X O X X X O O O |\n -------------------\nYou have won!\n","output_type":"stream"}]}]}